{"cells":[{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1713190665482,"user":{"displayName":"MARIO GARCIA HIDALGO","userId":"04558324611150804229"},"user_tz":-120},"id":"Yk9FDRMPUMWZ"},"outputs":[],"source":["# Avoid warnings from libraries\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Import libraries\n","from transformers import AutoProcessor, AutoTokenizer, CLIPModel\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","import transformers\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","import torch.nn.functional as F\n","from torch import nn\n","\n","from PIL import Image\n","\n","from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","import torch\n","import random\n","import re\n","import os\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","import matplotlib.pyplot as plt\n","import csv"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["parent_dir = # insert here the path of your parent dir\n","dataset_path = parent_dir + \"data/\"\n","models_dir = parent_dir + \"models/\"\n","results_csv =  parent_dir + \"results.csv\"\n","text_aug_path = parent_dir + \"data/text_augmentations/all_data.csv\""]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["model_name =   # insert the name of the model that will be loaded\n","results_path = models_dir+model_name+\"/\"\n","\n","# Multimodal model parameters\n","MAX_LENGTH = 48\n","encoding_dimension = 512\n","dense_hidden_size = 128\n","pretraining = \"openai/clip-vit-base-patch32\"\n","\n","fusion = \"cross\" # cross or concat\n","\n","# General training\n","batch_size = 32\n","lr = 1.5e-7\n","n_epochs = 4\n","n_classes = 3\n","patience = 3 # Number of epochs to wait before early stopping\n","\n","# Data augmentation\n","data_augmentation = True\n","\n","image_augmentations = transforms.Compose([\n","    transforms.RandomRotation(degrees=40),\n","    transforms.RandomAffine(degrees=0, translate=(0.4, 0.4), scale=(0.7, 1.3), shear=0),\n","    transforms.RandomHorizontalFlip(p=0.0),\n","    transforms.RandomVerticalFlip(p=0.0),\n","    transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0),\n","    transforms.RandomPerspective(distortion_scale=0.5)\n","])"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def get_random_seed_through_os():\n","    RAND_SIZE = 4\n","    random_data = os.urandom(\n","        RAND_SIZE\n","    )\n","    random_seed = int.from_bytes(random_data, byteorder=\"big\")\n","    return random_seed\n","\n","RANDOM_SEED = 123\n","# RANDOM_SEED = get_random_seed_through_os()\n","random.seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","torch.cuda.manual_seed_all(RANDOM_SEED)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["#Clean text and load the images names (as lists)\n","def clean_sentence(sentence, norm_user = True, norm_hashtag = True, separete_characters = True):\n","    # Convert instance to string\n","    sentence = str(sentence)\n","\n","    # All text to lowecase\n","    sentence = sentence.lower()\n","\n","    # Normalize users and url\n","    if norm_user == True:\n","        sentence = re.sub(r'\\@\\w+','@usuario', sentence)\n","    if norm_hashtag == True:\n","        sentence = re.sub(r\"http\\S+|www\\S+|https\\S+\", 'url', sentence, flags=re.MULTILINE)\n","\n","    # Separate special characters\n","    if separete_characters == True:\n","        sentence = re.sub(r\":\", \" : \", sentence)\n","        sentence = re.sub(r\",\", \" , \", sentence)\n","        sentence = re.sub(r\"\\.\", \" . \", sentence)\n","        sentence = re.sub(r\"!\", \" ! \", sentence)\n","        sentence = re.sub(r\"¡\", \" ¡ \", sentence)\n","        sentence = re.sub(r\"“\", \" “ \", sentence)\n","        sentence = re.sub(r\"'\", \" ' \", sentence)\n","        sentence = re.sub(r\"”\", \" ” \", sentence)\n","        sentence = re.sub(r\"\\(\", \" ( \", sentence)\n","        sentence = re.sub(r\"\\)\", \" ) \", sentence)\n","        sentence = re.sub(r\"\\?\", \" ? \", sentence)\n","        sentence = re.sub(r\"\\¿\", \" ¿ \", sentence)\n","\n","    # Substituting multiple spaces with single space\n","    sentence = re.sub(r'\\s+', ' ', sentence, flags=re.I)\n","\n","    return sentence"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1713190669697,"user":{"displayName":"MARIO GARCIA HIDALGO","userId":"04558324611150804229"},"user_tz":-120},"id":"89b3ibdQWhIs"},"outputs":[],"source":["current_epoch = 1\n","\n","# Defining the dataset class for loading the images and the associated text\n","class CustomDataset(Dataset):\n","    def __init__(self, image_folder, csv_file, mode):\n","        self.image_folder = image_folder\n","        self.data = pd.read_csv(csv_file)\n","        self.mode = mode\n","        \n","        # Load text tokenizer and image processor\n","        self.tokenizer = AutoTokenizer.from_pretrained(pretraining)\n","        self.image_processor = AutoProcessor.from_pretrained(pretraining)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        global current_epoch\n","        img_name = self.data.iloc[idx, 0]\n","        img_path = os.path.join(self.image_folder, img_name+ \".jpg\")\n","        \n","        image = Image.open(img_path).convert('RGB')\n","        \n","        # Data augmenting\n","        if data_augmentation and self.mode == \"train\":\n","            image = image_augmentations(image)\n","\n","        sentence = clean_sentence(self.data.iloc[idx, 1])\n","\n","        # Preprocess image according to CLIP\n","        inputs = self.image_processor(images=image, padding=True, return_tensors=\"pt\")\n","        image = inputs[\"pixel_values\"].squeeze(0)\n","\n","        # Tokenize the text\n","        encoded_dict = self.tokenizer(sentence, padding='max_length', max_length=MAX_LENGTH, truncation=True, return_tensors=\"pt\")\n","        input_ids = encoded_dict[\"input_ids\"].squeeze(0)\n","        attention_mask = encoded_dict[\"attention_mask\"].squeeze(0)\n","\n","\n","        return image, input_ids, attention_mask"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["class EarlyFusion(nn.Module):\n","    def __init__(self, encoding_dimension, dense_hidden_size, n_classes, fusion):\n","        super(EarlyFusion, self).__init__()\n","        self.clip_model = CLIPModel.from_pretrained(pretraining)\n","\n","        self.encoding_dimension = encoding_dimension\n","        self.dense_hidden_size = dense_hidden_size\n","        self.n_classes = n_classes\n","        \n","        self.fusion = fusion\n","        if self.fusion == \"cross\":\n","            self.features_dim = self.encoding_dimension**2\n","        elif self.fusion == \"concat\":\n","            self.features_dim = self.encoding_dimension*2\n","\n","        self.layer_norm = nn.LayerNorm(self.features_dim)\n","\n","        # MLP Classifier\n","        self.classifier = nn.Sequential(\n","            nn.Linear(self.features_dim, self.dense_hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(0.25),\n","            nn.Linear(self.dense_hidden_size, self.dense_hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(0.25),\n","            nn.Linear(self.dense_hidden_size, self.n_classes) \n","        )\n","\n","    def forward(self, image, input_ids, attention_mask):\n","        # Extract image and text features\n","        text_features = self.clip_model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n","        image_features = self.clip_model.get_image_features(image)\n","        \n","        if self.fusion == \"cross\":\n","            features = torch.bmm(image_features.unsqueeze(2), text_features.unsqueeze(1))\n","            features = features.reshape(features.shape[0], -1) \n","            norm_output = self.layer_norm(features)\n","        \n","        elif self.fusion == \"concat\":        \n","            concat_output = torch.cat((text_features, image_features), 1)\n","            norm_output = self.layer_norm(concat_output)\n","        \n","        out = self.classifier(norm_output)\n","\n","        return out"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3548,"status":"ok","timestamp":1713190673241,"user":{"displayName":"MARIO GARCIA HIDALGO","userId":"04558324611150804229"},"user_tz":-120},"id":"0vIep8Dtz5V-","outputId":"346ec850-d8e2-4828-d29b-7983fdfb3cff"},"outputs":[{"data":{"text/plain":["EarlyFusion(\n","  (clip_model): CLIPModel(\n","    (text_model): CLIPTextTransformer(\n","      (embeddings): CLIPTextEmbeddings(\n","        (token_embedding): Embedding(49408, 512)\n","        (position_embedding): Embedding(77, 512)\n","      )\n","      (encoder): CLIPEncoder(\n","        (layers): ModuleList(\n","          (0-11): 12 x CLIPEncoderLayer(\n","            (self_attn): CLIPAttention(\n","              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","            )\n","            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (mlp): CLIPMLP(\n","              (activation_fn): QuickGELUActivation()\n","              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","            )\n","            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","      )\n","      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (vision_model): CLIPVisionTransformer(\n","      (embeddings): CLIPVisionEmbeddings(\n","        (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n","        (position_embedding): Embedding(50, 768)\n","      )\n","      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (encoder): CLIPEncoder(\n","        (layers): ModuleList(\n","          (0-11): 12 x CLIPEncoderLayer(\n","            (self_attn): CLIPAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): CLIPMLP(\n","              (activation_fn): QuickGELUActivation()\n","              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","      )\n","      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n","    (text_projection): Linear(in_features=512, out_features=512, bias=False)\n","  )\n","  (layer_norm): LayerNorm((262144,), eps=1e-05, elementwise_affine=True)\n","  (classifier): Sequential(\n","    (0): Linear(in_features=262144, out_features=128, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.25, inplace=False)\n","    (3): Linear(in_features=128, out_features=128, bias=True)\n","    (4): ReLU()\n","    (5): Dropout(p=0.25, inplace=False)\n","    (6): Linear(in_features=128, out_features=3, bias=True)\n","  )\n",")"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# Loading images and text form the dataset\n","# train_dataset = CustomDataset(dataset_path, text_aug_path, mode=\"train\")\n","# val_dataset = CustomDataset(dataset_path + \"val_images\", dataset_path + \"validation_data_task_2.csv\", mode=\"val\")\n","test_dataset = CustomDataset(dataset_path + \"test_images\", dataset_path + \"test_data.csv\", mode=\"val\")\n","\n","# Defining data loaders\n","# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Instantiating the multimodal model\n","multi_model = EarlyFusion(encoding_dimension, dense_hidden_size, n_classes, fusion)\n","\n","# Train set up\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","multi_model.to(device)\n","# optimizer = AdamW(multi_model.parameters(), lr)\n","\n","# Define the number of training steps (epochs * number of batches)\n","# total_steps = len(train_loader) * n_epochs\n","# # Create a schedule for the LR update\n","# scheduler = get_linear_schedule_with_warmup(optimizer,\n","#                                             num_warmup_steps = 0,\n","#                                             num_training_steps = total_steps)\n","\n","# Uncomment for adding class weights\n","#-------------------------------------\n","# class_weights = compute_class_weight('balanced', classes=np.unique(train_dataset.data['encoded_labels']), y=train_dataset.data['encoded_labels'])\n","# print(\"Class weights: \", class_weights)\n","# class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n","# criterion = nn.CrossEntropyLoss(weight=class_weights)\n","\n","# criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"elapsed":118780,"status":"error","timestamp":1713190660849,"user":{"displayName":"MARIO GARCIA HIDALGO","userId":"04558324611150804229"},"user_tz":-120},"id":"UScernwjjIa3","outputId":"80b2c02b-a65e-46c9-bb41-d9b3a76b62db"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2, 1, 1, 2, 1, 0, 2, 1, 0, 0, 1, 1, 2, 0, 0, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 2, 0, 0, 2, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 1, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 1, 2, 1, 2, 0, 2, 2, 2, 1, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 0, 2, 1, 0, 0, 2, 2, 0, 2, 2, 0, 2, 1, 0, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 0, 1, 2, 0, 2, 2, 2, 2, 0, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 0, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 0, 1, 2, 2, 1, 2, 2, 0, 2, 1, 2, 2, 1, 2, 1, 2, 2, 0, 0, 0, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 0, 2, 2, 2, 1, 1, 1, 0, 2, 2, 2, 1, 1, 2, 2, 0, 1, 2, 2, 2, 0, 2, 1, 0, 2, 2, 2, 0, 2, 2, 2, 1, 2, 0, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 0, 0, 2, 2, 2, 1, 2, 0, 1, 2, 2, 0, 2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 2, 2, 0, 2, 2, 1, 1, 0, 2, 2, 2, 2, 2, 2, 2, 1, 0, 1, 1, 2, 2, 2, 0, 2, 0, 2, 1, 1, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 0, 2, 1, 2, 2, 2, 1, 1, 0, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 0, 2, 1, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 0, 1, 0, 2, 2, 0, 0, 1, 2, 1, 0, 2, 2, 1, 2, 2, 1, 2, 0, 2, 0, 2, 2, 1, 1, 2, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 2, 0, 0, 2, 2, 1, 1, 1, 2, 2, 1, 0, 2, 2, 2, 2, 0, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 0, 0, 0, 2, 2, 1, 1, 0, 0, 2, 2, 2, 2, 2, 2, 1, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 0, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 0, 0, 2, 2, 1, 1, 2, 2, 2, 0, 1, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 2, 1, 1, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 0, 1, 2, 2, 2, 0, 2, 1, 2, 1, 0, 0, 2, 2, 2, 2, 2, 1, 0, 2, 0, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 0, 2, 1, 2, 1, 0, 1, 2, 2, 2, 0, 0, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 0, 2, 2, 0, 2, 2, 2, 0, 2, 0, 2, 1, 2, 2, 2, 0, 2, 0]\n","Vectores one-hot escritos en CLIP_base_Raug_cross_lr78_finalModel.csv\n"]}],"source":["# Evaluación del modelo en el conjunto de prueba\n","test_predictions = []\n","\n","# recover model best state\n","multi_model.load_state_dict(torch.load(results_path + \"best_model.pt\"))\n","\n","with torch.no_grad():\n","      for images, input_ids, attention_masks in test_loader:\n","        images = images.to(device)\n","        input_ids = input_ids.to(device)\n","        attention_masks = attention_masks.to(device)\n","\n","        outputs = multi_model(images, input_ids, attention_masks)\n","        _, predicted = torch.max(outputs, 1)\n","\n","        test_predictions.extend(predicted.cpu().numpy())\n","\n","print(test_predictions)\n","\n","# Función para convertir un valor a un vector one-hot\n","def valor_a_one_hot(valor, num_clases):\n","    one_hot = np.zeros(num_clases, dtype=int)\n","    one_hot[valor] = 1\n","    return one_hot\n","\n","# Número de clases (en este caso, 3: 0, 1 y 2)\n","num_clases = 3\n","\n","# Nombre del archivo CSV de salida\n","nombre_archivo_csv = str(model_name) + '.csv'\n","\n","# Abrir el archivo CSV en modo escritura\n","with open(nombre_archivo_csv, mode='w', newline='') as archivo_csv:\n","    escritor_csv = csv.writer(archivo_csv)\n","\n","    # Iterar sobre la lista de valores y escribir cada vector one-hot en el CSV\n","    for valor in test_predictions:\n","        vector_one_hot = valor_a_one_hot(valor, num_clases)\n","        escritor_csv.writerow(vector_one_hot)\n","\n","print(f'Vectores one-hot escritos en {nombre_archivo_csv}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
